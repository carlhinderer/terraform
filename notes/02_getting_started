-----------------------------------------------------------------------
| CHAPTER 2 - GETTING STARTED WITH TERRAFORM                          |
-----------------------------------------------------------------------

- Setting Up AWS Account

    - We'll log into our root user account.  The only thing we should use the root user account for is 
        to create other user accounts with more limited permissions, then switch to these user accounts
        immediately.


    - To create a more limited user account:

        - IAM Service
            > Add User
                > Name: terraform-user
                > Access Key - Programmatic Access


    - By default, new IAM users have no permissions.  To give the user permissions, you need to assoicate
        one or more IAM policies with the user's account.  An 'IAM Policy' is a JSON document that defines
        what the user is or isn't allowed to do.  You can create your own IAM policies or use the predefined
        policies built into your account ('Managed Policies').

      To run examples in this book, the easiest way is to add the 'AdministratorAccess' Managed Policy to
        your IAM user.


    - Create the user, then copy the 'Access Key Id' and 'Secret Access Key', since they will never be
        shown again.  Consider saving them in LastPass or some other password manager.


    - All of the AWS examples in this book use the 'Default VPC' in your account.  

        - A 'VPC' (Virtual Private Cloud) is an isolated area of your AWS account that has it's own virtual
            network and IP address space.  

        - Just about every AWS resource deploys into a VPC.  If you don't explicitly specify a VPC, the
            resource will be deployed into the default VPC.



- Installing Terraform

    - The easiest way to install Terraform is to use your OS's package manager.

        # Homebrew on MacOS
        $ brew tap hashicorp/tap
        $ brew install hashicorp/tap/terraform


    - To install on Ubuntu using APT

        # Install required dependencies
        $ apt-get install wget curl unzip software-properties-common gnupg2 -y

        # Add the Hashicorp repository to apt
        $ apt-add-repository "deb [arch=$(dpkg --print-architecture)] https://apt.releases.hashicorp.com $(lsb_release -cs) main"

        # Update the repository
        $ apt-get update -y

        # Install Terraform
        $ apt-get install terraform -y

        # Verify installation
        $ terraform -v


    - To install the auto-complete Terraform extension:

        $ terraform -install-autocomplete
        $ source ~/.bashrc


    - In order for Terraform to make changes to your AWS account, you'll need to set the creds for the IAM
        account you created earlier:

        $ export AWS_ACCESS_KEY_ID=(your access key id)
        $ export AWS_SECRET_ACCESS_KEY=(your secret access key)

      Alternatively, Terraform supports the same authentication mechanisms as all AWS CLI and SDK tools.
        It can use credentials in $HOME/.aws/credentials, which are automatically generated if you run
        'aws configure', or IAM roles, which you can add to almost any resource in AWS.



- Deploying a Single Server

    - Terraform code is written in HCL, in files with the extension '.tf'.  Terraform can create 
        infrastructure across a variety of platforms (which is calls 'providers').


    - The first step is typically to configure the provider you want to use.  Create an empty folder and
        put a file called 'main.tf' into it:

        # main.tf
        ---------------------
        provider "aws" {
          region = "us-east-2"
        }


    - AWS has datacenters all over the world, grouped into regions (ie us-east-2 or eu-west-1).  Within each 
        region, there are multiple isolated datacenters known as Availability Zones or AZ's (ie us-east-2a
        or us-east-2b).


    - For each type of provider, there are many different types of 'resources' you can create, such as
        servers, databases, and load balancers.  The general syntax for creating a resource is:

        resource "<PROVIDER>_<TYPE>" "<NAME>" {
          [CONFIG ...]
        }

        PROVIDER      # Name of provider
        TYPE          # Resource to create in the provider
        NAME          # Identifier that can be used throughout Terraform code
        CONFIG        # Arguments specific to the resource


    - Here is an example of deploying a single EC2 instance, which we'll add to 'main.tf'.

        # main.tf
        ---------------------
        resource "aws_instance" "example" {
          ami           = "ami-0fb653ca2d3203ac1"
          instance_type = "t2.micro"
        }


    - An AMI (Amazon Machine Instance) is the server template used to run an EC2 instance.  There are free
        and paid AMIs in the AWS Marketplace.  You can also create your own AMIs using tools like Packer.
        Note that the AMI IDs are different in every AWS region, so you need to look up the corresponding
        IDs if you use a different region.


    - Next, we'll run the 'terraform init' command, which scans your code, figures out which providers
        you are using, and downloads the code for them into a '.terraform' folder.  Terraform will also
        record which versions of the provider code it downloaded into a '.terraform.lock.hcl' file.  The
        'init' command is idempotent, so it can be run multiple times.

        $ cd code
        $ terraform init


    - Now, we can run the 'plan' command to see what will be done before making any actual changes.

        $ terraform plan

        # Plan lines
        +     Will be created
        -     Will be deleted
        ~     Will be modified in place


    - To actually create the instance, run the 'apply' command.

        $ terraform apply


    - We now have our EC2 instance running, but it doesn't have a name.  To add one, we'll add a tag to
        the resource, then run 'apply' to add the name.

        # main.tf
        --------------
        resource "aws_instance" "example" {
          ami = "ami-0fb653ca2d3203ac1"
          instance_type = "t2.micro"

          tags = {
            Name = "terraform-example"
          }
        }


    - Terraform keeps track of all the resources it already created for this set of config files, so it
        knows your EC2 instance already exists and modifies it in place.



- Deploying a Single Web Server

    - For the simplest possible web server, we'll use a tool called 'busybox' that is installed by default
        on Ubuntu.  We'll wrap the busybox command with a 'nohup' and '&' so that the web server runs
        permanently in the background whereas the bash script itself can exit.

        #!/bin/bash
        echo "Hello, World" > index.html
        nohup busybox httpd -f -p 8080 &


    - Note that we use port 8080 instead of 80, because listening on any port less than 1024 requires
        root user privileges.  This is a security risk, since any attacker who manages to compromise your
        server would get root privileges, too.  Therefore, it is a best practice to listen on 
        higher-numbered ports.


    - Normally, a tool like Packer would be used to create a custom AMI that has a web server installed on
        it.  For this simple web server, we'll just add our shell script to our EC2 intance's User Data,
        which will be executed by the EC2 instance during it's first boot.

      We'll use the Terraform 'user_data_replace_on_true' setting so that when the 'user_data' paremter is
        changed, the original instance will be terminated and a new one will be created.

        resource "aws_instance" "example" {
          ami           = "ami-0fb653ca2d3203ac1"
          instance_type = "t2.micro"

          user_data = <<-EOF
                      #!/bin/bash
                      echo "Hello, World" > index.html
                      nohup busybox httpd -f -p 8080 &
                      EOF

          user_data_replace_on_change = true

          tags = {
            Name = "terraform-example"
          }
        }


    - By default, AWS does not allow any incoming or outgoing traffic from an EC2 instance.  To allow the
        EC2 instance to receive traffic on port 8080, we need to create a security group:

        resource "aws_security_group" "instance" {
          name = "terraform-example-instance"

          ingress {
            from_port = 8080
            to_port = 8080
            protocol = "tcp"
            cidr_blocks = ["0.0.0.0/0"]
          }
        }


    - This security group allows incoming TCP requests on port 8080 from the CIDR block 0.0.0.0/0.  CIDR
        blocks are a concise way to specify IP address ranges.  For example, a CIDR block of 10.0.0.0/24
        represents all IP addresses between 10.0.0.0 and 10.0.0.255.  The CIDR block 0.0.0.0/0 is an IP
        address range that includes all possible IP addresses.


    - Now, we also need to tell our EC2 instance to use the new security group by passing the ID of the
        security group.  To do that, we'll need to use a Terraform expression.

        - An 'expression' in Terraform is anything that returns a value.  So far, we have used only 
            literals.

        - One particularly useful type of expression is a reference, which takes the form:
            <PROVIDER>_<TYPE>.<NAME>.<ATTRIBUTE> (ie aws_security_group.instance.id)


    - We'll add this attribute to our EC2 instance resource:

        vpc_security_group_ids = [aws_security_group.instance.id]


    - When you add a reference from one resource to another, you create an 'implicit dependency'.
        Terraform parses these dependencies, builds a dependency graph from them, and uses it to 
        determine which order to create resources in.  To see the dependency graph:

        $ terraform graph

      The output is in a graph description language called DOT, so you can use it to create an image of
        the graph using a tool like Graphviz.


    - Now, we can run 'apply' to create the new security group and re-create the EC2 instance.  Note that
        while the web server is being replaced, users will experience downtime.  We'll learn how to do a
        zero-downtime deployment later.


    - If we click our new instance in the console, we can get it's public IP address from the description
        panel.  We can use a web browser or a tool like curl to make a request to this IP address.

        $ curl http://<EC2_INSTANCE_PUBLIC_IP>:8080



- Network Security

    - To keep the examples in this book simple, all deploy into the Default VPC into the default
        subnets of that VPC.

    - A VPC is partitioned into one or more subnets, each of which has it's own IP addresses.

    - The subnets in the Default VPC are all public subnets, which means they get IP addresses that
        are accessible from the public internet.  This is why we can test our EC2 instances from our home 
        computer.

    - Running a server in a public subnet like this is a security risk in the real world.  Hackers are
        constantly scanning IP addresses at random for any weakness.  If your servers are exposed publicly,
        leaving a single port unprotected or running out-of-date code can allow someone to break in.

    - Therefore, for all production servers, you should deploy all of your servers and data stores in
        private subnets, which have IP addresses that can be accessed only from within the VPC and not
        from the public internet.

    - The only servers you should run in public subnets are a small number of reverse proxies and load
        balancers that you lock down as much as possible.



- Deploying a Configurable Web Server

    - Right now, we have the port 8080 duplicated in both the security group and User Data configuration.
        To DRY this out, we can define an 'input variable'.

        variable "NAME" {
          [CONFIG ...]
        }


    - The body of the variable can have the following optional parameters:

        description        # Documentation for how the variable is used
        default            # Default value if not passed in via command line or environment variable
        type               # Type constraint on the variable a user passes in
        validation         # Custom validation rules on value passed in
        sensitive          # Won't be logged with 'plan' or 'apply' commands


    - Example: Checks whether value is a number

        variable "number_example" {
          description = "An example of a number variable in Terraform"
          type = number
          default = 42
        }


    - Example: Checks whether value is a list

        variable "list_example" {
          description = "An example of a list in Terraform"
          type = list
          default = ["a", "b", "c"]
        }


    - Example: Checks whether value is a list of numbers

        variable "list_numeric_example" {
          description = "An example of a numeric list in Terraform"
          type = list(number)
          default = [1, 2, 3]
        }


    - Example: Checks whether value is a map of strings:

        variable "map_example" {
          description = "An example of a map in Terraform"
          type = map(string)

          default = {
            key1 = "value1"
            key2 = "value2"
            key3 = "value3"
          }
        }


    - Example: Create more complex structural type:

        variable "object_example" {
          description = "An example of a structural type in Terraform"

          type = object({
            name = string
            age = number
            tags = list(string)
            enabled = bool
          })

          default = {
            name = "value1"
            age = 42
            tags = ["a", "b", "c"]
            enabled = true
          }
        }


    - We'll create a variable that stores the web server port number:

        variable "server_port" {
          description = "The port number the server will use for HTTP requests"
          type        = number
        }


    - Now, when run 'apply', since our variable has no default, we will be prompted for a value
        interactively.

        $ terraform apply


    - Alternatively, we can provide a command-line value when we run the command:

        $ terraform plan -var "server_port=8080"


    - Or we can set the variable via an environment variable named TF_VAR_<name>.

        $ export TF_VAR_server_port=8080
        $ terraform plan


    - In our case, we'll provide a default value so that we don't have to pass it in every time:

        variable "server_port" {
          description = "The port number the server will use for HTTP requests"
          type        = number
          default     = 8080
        }


    - We can refer to the new variable in our security group resource using the var.<VARIABLE_NAME> variable
        reference syntax:

        ingress {
          from_port = var.server_port
          to_port = var.server_port
          protocol = "tcp"
          cidr_blocks = ["0.0.0.0/0"]
        }


    - We can also refer to the new variable in our User Data using the ${...} interpolation expression
        syntax:

        user_data = <<-EOF
                    #!/bin/bash
                    echo "Hello, World" > index.html
                    nohup busybox httpd -f -p ${var.server_port} &
                    EOF



- Output Variables

    - In addition to input variables, we can also define output variables:

        output "<NAME>" {
          value = <VALUE>
          [CONFIG ...]
        }

      The CONFIG can have these optional parameters:

        description        # Documentation
        sensitive          # Will not be logged with plan or apply
        depends_on         # Extra hint for dependency graph if needed


    - For instance, instead of having to manually look for the IP address of our EC2 instance, we can
        provide it with an output variable:

        output "public_ip" {
          value       = aws_instance.example.public_ip
          description = "The public IP address of the web server"
        }


    - If we now run 'apply', there won't be any changes, but the IP address will be output.

        $ terraform apply


    - We can also use the 'output' command to see the value of a specific output.  This is handy for
        scripting.  For instance, you could run 'terraform apply' in a script, use the 'output' command
        to grab the public IP, then run curl on the IP as a quick smoke test to validate that the deployment
        worked.

        $ terraform output public_ip



- Deploying a Cluster of Web Servers

    - In production, we will run multiple servers, scaling the number up and down based on traffic.  AWS
        can handle this scaling automatically using an ASG (Auto Scaling Group), which will monitor health
        of your servers, replace failed instances, and adjust the cluster size according to load.


    - The first step in creating an ASG is to create a 'launch configuration', which specifies how to
        configure each EC2 instance in the ASG.  This is similar to our EC2 instance resource, but the 'ami'
        and 'vpc_security_group_ids' parameters are changed to 'image_id' and 'security_groups'.  Also,
        the launch configuration doesn't support tags.  We'll replace 'aws_instance' with this launch 
        configuration:

        resource "aws_launch_configuration" "example" {
          image_id        = "ami-0fb653ca2d3203ac1"
          instance_type   = "t2.micro"
          security_groups = [aws_security_group.instance.id]

          user_data = <<-EOF
                      #!/bin/bash
                      echo "Hello, World" > index.html
                      nohup busybox httpd -f -p ${var.server_port} &
                      EOF
        }


    - Note that these days, we should actually be using a 'launch template' rather than a launch
        configuration.  However, we're using launch configurations in this book to keep examples simpler.


    - Now that we have the launch configuration, we can create the ASG itself:

        resource "aws_autoscaling_group" "example" {
          launch_configuration = aws_launch_configuration.example.name

          min_size = 2
          max_size = 10

          tag {
            key = "Name"
            value = "terraform-asg-example"
            propagate_at_launch = true
          }
        }


    - Note that the ASG uses a reference to fill in the launch configuration name.  This leads to a problem.
        Since launch configurations are immutable, if you change any parameter of your launch configuration,
        Terraform will try to replace it by deleting the old resource and creating it's replacement.
        However, since our ASG now has a reference to the old resource, Terraform won't be able to delete it.


    - To solve this problem, we can use a 'lifecycle' setting.  Every Terraform resource supports several
        lifecycle settings that specify how the resource is created/updated/deleted.

      One useful lifecycle setting is 'create_before_destroy', which will invert the order in which 
          resources are replaced.  We'll add this setting to our lanuch configuration:

          resource "aws_launch_configuration" "example" {
            ...

            # Required when using a launch configuration with an auto scaling group.
            lifecycle {
              create_before_destroy = true
            }
          }


    - There is one other parameter we need to add to our ASG to make it work: 'subnet_ids'.  This specifies
        the VPC subnet into which EC2 instances should be deployed.

        - Each subnet lives in an isolated AZ (datacenter), so by deploying your instances across multiple
            subnets, you can ensure your service will keep running even if there is an outage.

        - You could hardcode the list of subnets, but a better option is to use 'data sources' to get the
            list of subnets in your AWS account.


    - A 'data source' is a piece of read-only information that is fetched from a provider every time you
        run Terraform.  It's a way to query the provider's APIs for data and make it available to your code.
        The syntax for using a data source is:

        data "<PROVIDER>_<TYPE>" "<NAME>" {
          [CONFIG ...]
        }


    - Here, we use the 'aws_vpc' data source to look up data for our Default VPC:

        data "aws_vpc" "default" {
          default = true
        }

      Note that the arguments we pass in with data sources are typically search filters.


    - To get data out of a data source, you use this syntax:

        data.<PROVIDER>_<TYPE>.<NAME>.<ATTRIBUTE>
        data.aws_vpc.default.id


    - We can combine this with another data source, 'aws_subnets', to look up the subnets within that
        VPC:

        data "aws_subnets" "default" {
          filter {
            name = "vpc-id"
            values = [data.aws_vpc.default.id]
          }
        }


    - Finally, you can pull the subnet IDs out of the 'aws_subnets' data source and tell your ASG to use
        those subnets via the 'vpc_zone_identifier' argument:

        resource "aws_autoscaling_group" "example" {
          launch_configuration = aws_launch_configuration.example.name
          vpc_zone_identifier = data.aws_subnets.default.ids

          min_size = 2
          max_size = 10

          tag {
            key = "Name"
            value = "terraform-asg-example"
            propagate_at_launch = true
          }
        }



- Deploying a Load Balancer

    - At this point, we can deploy our ASG, but each server will have it's own IP address.  We can solve
        this problem by deploying a load balancer.  We'll use AWS's ELB (Elastic Load Balancer) service,
        which is highly available and scalable.


    - AWS offers 3 types of load balancers:

        1. ALB (Application Load Balancer)
             - Best suited for HTTP and HTTPS traffic
             - Operates at OSI Layer 7

        2. NLB (Network Load Balancer)
             - Best suited for TCP, UDP, TLS traffic
             - Can scale up and down faster that ALB
             - Operates at OSI Layer 4

        3. CLB (Classic Load Balancer)
             - Legacy load balancer that predates ALB and NLB
             - Can handle HTTP, HTTPS, TCP, TLS
             - Operates at both Layer 7 and Layer 4


    - Since we're using web servers, we'll use ALB.  ALB consists of several parts:

        - Listener = Listens on a specific port and protocol

        - Listener rule = Takes requests that come into a listener and sends those that match specific
                            paths (ie /foo) or hostnames (ie foo.example.com) to specific target groups.

        - Target groups = One or more servers that receive requests from the load balancer.  The target
                            group also perform health checks and only sends requests to healthy nodes.


    - First, we'll create the ALB itself.

        resource "aws_lb" "example" {
          name = "terraform-asg-example"
          load_balancer_type = "application"
          subnets = data.aws_subnets.default.ids
        }


    - Note that the 'subnets' parameter configures the load balancer to use all the subnets in your
        Default VPC.  AWS load balancers don't consist of a single server, but of multiple servers that
        can run in separate subnets.  AWS automatically scales the number of load balancer servers up and
        down based on traffic and handles failover if one of those servers goes down, so you get
        scalability and high availability out of the box.


    - Next, we'll define a listener for this ALB:

        resource "aws_lb_listener" "http" {
          load_balancer_arn = aws_lb.example.arn
          port              = 80
          protocol          = "HTTP"

          # By default, return a simple 404 page
          default_action {
            type = "fixed-response"

            fixed_response {
              content_type = "text/plain"
              message_body = "404: page not found"
              status_code = 404
            }
          }
        }


    - By default, all AWS resources, including ALBs, don't allow any incoming or outgoing traffic, so you
        need to create a new security group specifically for the ALB.  This security group should allow
        incoming requests on port 80 so that you can access the load balancer over HTTP, and outgoing
        requests on all ports so that the load balancer can perform health checks.

        resource "aws_security_group" "alb" {
          name = "terraform-example-alb"

          # Allow inbound HTTP requests
          ingress {
            from_port = 80
            to_port = 80
            protocol = "tcp"
            cidr_blocks = ["0.0.0.0/0"]
          }

          # Allow all outbound requests
          egress {
            from_port = 0
            to_port = 0
            protocol = "-1"
            cidr_blocks = ["0.0.0.0/0"]
          }
        }


    - We'll need to tell the 'aws_lb' resource to use this security group:

        resource "aws_lb" "example" {
          name = "terraform-asg-example"
          load_balancer_type = "application"
          subnets = data.aws_subnets.default.ids
          security_groups = [aws_security_group.alb.id]
        }


    - Next, we'll create a target group for our ASG:

        resource "aws_lb_target_group" "asg" {
          name = "terraform-asg-example"
          port = var.server_port
          protocol = "HTTP"
          vpc_id = data.aws_vpc.default.id

          health_check {
            path = "/"
            protocol = "HTTP"
            matcher = "200"
            interval = 15
            timeout = 3
            healthy_threshold = 2
            unhealthy_threshold = 2
          }
        }


    - How does the target group know which EC2 instances to send requests to?  You could attach a static
        list of EC2 instances, but with an ASG, instances can launch or terminate at any time, so this 
        won't work.  Instead, we'll take advantage of the integration between ASG and ALB, and update
        the 'aws_autoscaling_group resource':

        resource "aws_autoscaling_group" "example" {
          ...

          target_group_arns = [aws_lb_target_group.asg.arn]
          health_check_type = "ELB"
        }